{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scientists often need to crawl data from websites and turn the crawled data (HTML pages) to structured data (tables). Thus, web scraping is an essential skill that every data scientist should master. In this assignment, you will learn the followings:\n",
    "\n",
    "\n",
    "* How to use [requests](http://www.python-requests.org/en/master/) to download HTML pages from a website?\n",
    "* How to select content on a webpage with [lxml](http://lxml.de/)? \n",
    "\n",
    "You can either use Spark DataFrame or [pandas.DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) to do the assignment. In comparison, pandas.DataFrame has richer APIs, but is not good at distributed computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this is your first time to write a web scraper, you need to learn some basic knowledge of HTML, DOM, and XPath. I found that this is a good resource: [https://data-lessons.github.io/library-webscraping/](https://data-lessons.github.io/library-webscraping/). Please take a look at\n",
    "\n",
    "* [Selecting content on a web page with XPath\n",
    "](https://data-lessons.github.io/library-webscraping/xpath/)\n",
    "* [Web scraping using Python: requests and lxml](https://data-lessons.github.io/library-webscraping/04-lxml/). \n",
    "\n",
    "Please let me know if you find a better resource. I'll share it with the other students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are a data scientist working at SFU. One day, you want to analyze CS faculty data and answer two interesting questions:\n",
    "\n",
    "1. Who are the CS faculty members?\n",
    "2. What are their research interests?\n",
    "\n",
    "To do so, the first thing is to figure out what data to collect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: SFU CS Faculty Members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You find that there is a web page in the CS school website, which lists all the faculty members as well as their basic information. \n",
    "\n",
    "In Task 1, your job is to write a web scraper to extract the faculty information from this page: [https://www.sfu.ca/computing/people/faculty.html](https://www.sfu.ca/computing/people/faculty.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Crawling Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A web page is essentially a file stored in a remote machine (called web server). You can use [requests](http://www.python-requests.org/en/master/) to open such a file and read data from it. Please complete the following code to download the HTML page and save it as a text file (like [this](./faculty.txt)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Download the webpage\n",
    "response = requests.get('https://www.sfu.ca/computing/people/faculty.html')\n",
    "\n",
    "# 2. Save it as a text file (named faculty.txt)\n",
    "file = open('faculty.txt', 'w')\n",
    "file.write(response.text)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Extracting Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An HTML page follows the Document Object Model (DOM). It models an HTML page as a tree structure wherein each node is an object representing a part of the page. The nodes can be searched and extracted programmatically using XPath. Please complete the following code to transform the above HTML page to a CSV file (like [this](./faculty_table.csv)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html \n",
    "\n",
    "# 1. Open faculty.txt\n",
    "def readfromfile(filename):\n",
    "\twith open(filename, 'r') as content_file:\n",
    "\t\tfilecontent = content_file.read()\n",
    "\t\treturn(filecontent)\n",
    "\n",
    "filecontent = readfromfile('faculty.txt')\n",
    "\n",
    "# 2. Parse the HTML page as a tree structure\n",
    "doc = lxml.html.fromstring(filecontent)\n",
    "nodepath = \"//div[@class='textimage section']//div[@class='text']\"\n",
    "SFU_HOMEPAGE = 'http://www.sfu.ca'\n",
    "\n",
    "# 3. Extract related content from the tree using XPath\n",
    "sfu_prof_records = []\n",
    "\n",
    "prof_els = doc.xpath(nodepath)\n",
    "for prof_el in prof_els:\n",
    "\t\tname = ''\n",
    "\t\tdesignation = ''\n",
    "\t\tprofile = ''\n",
    "\t\thomepage = ''\n",
    "\t\tresearch_interests = []\n",
    "\n",
    "\t\tfor children in prof_el.getiterator():\n",
    "\t\t\tif(children.tag  == 'h4'):\n",
    "\t\t\t\tprof = (children.text_content().split('\\n'))[0].split(',')\n",
    "\t\t\t\tprof_name = prof[0]\n",
    "\t\t\t\tname = prof_name.title()\n",
    "\t\t\t\tprof_designation = prof[1]\n",
    "\t\t\t\tprof_designation = prof_designation.strip()\n",
    "\t\t\t\tdesignation = prof_designation.title()\n",
    "\t\t\t\tif name == 'Associate Director': continue\n",
    "# \t\t\t\tprint('Name: ' + name)\n",
    "# \t\t\t\tprint('Designation:' + prof_designation.title())\n",
    "\n",
    "\t\t\telif (children.tag == 'p'):\n",
    "\t\t\t\tfor profile_details in children.getiterator():\n",
    "\t\t\t\t\tlink = profile_details.get('href')\n",
    "\t\t\t\t\ttitle = profile_details.text_content()\n",
    "\t\t\t\t\tif title == 'Profile & Contact Information' and link is not None:\n",
    "\t\t\t\t\t\tif not link.startswith(SFU_HOMEPAGE): link = SFU_HOMEPAGE + link\n",
    "\t\t\t\t\t\t# print('Profile:' + link)\n",
    "\t\t\t\t\t\tprofile = link\n",
    "\t\t\t\t\telif (title == 'Home Page'):\n",
    "\t\t\t\t\t\thomepage = link\n",
    "\t\t\t\t\t\t# print('HomePage:' + homepage)\n",
    "\t\t\n",
    "\t\trow_tuple = (name, designation, profile, homepage)\n",
    "\t\tsfu_prof_records.append(row_tuple)\n",
    "\n",
    "# 4. Save the extracted content as an csv file (named faculty_table.csv)\n",
    "import csv\n",
    "\n",
    "with open('faculty_table.csv','w') as out:\n",
    "\tcsv_out=csv.writer(out)\n",
    "\tcsv_out.writerow(['name', 'designation', 'profile', 'homepage'])\n",
    "\tfor row in sfu_prof_records:\n",
    "\t\tcsv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Research Interests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to know the research interests of each faculty. However, the above crawled web page does not contain such information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Crawling Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You notice that such information can be found on the profile page of each faculty. For example, you can find the research interests of Dr. Jiannan Wang from [http://www.sfu.ca/computing/people/faculty/jiannanwang.html](http://www.sfu.ca/computing/people/faculty/jiannanwang.html). \n",
    "\n",
    "\n",
    "Please complete the following code to download the profile pages and save them as text files. There are 56 faculties, so you need to download 56 web pages in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Download the profile pages of 56 faculties\n",
    "for prof in sfu_prof_records:\n",
    "    (name, designation, profile, homepage) = prof\n",
    "#     print(profile)\n",
    "\n",
    "# 2. Save each page as a text file\n",
    "    response = requests.get(profile)\n",
    "    file = open(name + '.txt', 'w')\n",
    "    file.write(response.text)\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Extracting Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the following code to extract the research interests of each faculty, and generate a file like [this](./faculty_more_table.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html \n",
    "\n",
    "# 1. Open each text file and parse it as a tree structure \n",
    "def readfromfile(filename):\n",
    "\twith open(filename, 'r') as content_file:\n",
    "\t\tfilecontent = content_file.read()\n",
    "\t\treturn(filecontent)\n",
    "\n",
    "def getinterests(filename):\n",
    "    content = readfromfile(filename)\n",
    "    doc = lxml.html.fromstring(content)\n",
    "\n",
    "    li_sections = doc.xpath(\"//div[@class='text parbase section']//div[@class='ruled']\")\n",
    "    if not li_sections:\n",
    "        li_sections = doc.xpath(\"//div[@class='text parbase section']//div[@class='listed']\")\n",
    "\n",
    "    for section in li_sections:\n",
    "        tables = section.getchildren()\n",
    "        if tables:\n",
    "            h2 = tables[0]\n",
    "            if h2.text_content().strip().lower() == 'research interests':\n",
    "                ul = tables[1]\n",
    "                interests = ul.text_content().split('\\n')\n",
    "                interests = list(filter(None, interests))\n",
    "                return(interests)\n",
    "\n",
    "# 2. Extract the research interests from each tree using XPath\n",
    "faculty_more_tuples = []\n",
    "\n",
    "# 3. Add the extracted content to faculty_table.csv    \n",
    "# 4. Generate a new CSV file, named faculty_more_table.csv\n",
    "import csv\n",
    "with open('faculty_more_table.csv','w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['name', 'designation', 'profile', 'homepage', 'research_interests'])\n",
    "    for prof in sfu_prof_records:\n",
    "        (name, designation, profile, homepage) = prof\n",
    "        filename = (name + '.txt')\n",
    "        interests = getinterests(filename)\n",
    "        row_tuple = (name, designation, profile, homepage, interests)\n",
    "        csv_out.writerow(row_tuple)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete python code\n",
    "\n",
    "I have also included a .py file that I written initially that can be run as a standalone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import lxml.html, requests\n",
    "import csv\n",
    "\n",
    "def savetoTXTfile(filename, content):\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(content)\n",
    "\tfile.close()\n",
    "\n",
    "def savetoCSVfile(filename, content):\n",
    "\twith open(filename,'w') as out:\n",
    "\t\tcsv_out=csv.writer(out)\n",
    "\t\tcsv_out.writerow(['name', 'designation', 'profile', 'homepage', 'research_interests'])\n",
    "\t\tfor row in content:\n",
    "\t\t\tcsv_out.writerow(row)\n",
    "\n",
    "def readfromfile(filename):\n",
    "\twith open(filename, 'r') as content_file:\n",
    "\t\tfilecontent = content_file.read()\n",
    "\t\treturn(filecontent)\n",
    "\n",
    "def getinterests(profile_url):\n",
    "\tresponse = requests.get(profile_url)\n",
    "\tcontent = response.text\n",
    "\tdoc = lxml.html.fromstring(content)\n",
    "\n",
    "\tli_sections = doc.xpath(\"//div[@class='text parbase section']//div[@class='ruled']\")\n",
    "\tif not li_sections:\n",
    "\t\tli_sections = doc.xpath(\"//div[@class='text parbase section']//div[@class='listed']\")\n",
    "\n",
    "\tfor section in li_sections:\n",
    "\t\ttables = section.getchildren()\n",
    "\t\tif tables:\n",
    "\t\t\th2 = tables[0]\n",
    "\t\t\tif h2.text_content().strip().lower() == 'research interests':\n",
    "\t\t\t\tul = tables[1]\n",
    "\t\t\t\tinterests = ul.text_content().split('\\n')\n",
    "\t\t\t\tinterests = list(filter(None, interests))\n",
    "\t\t\t\treturn(interests)\n",
    "\n",
    "\n",
    "def makelocalcopyofpage(url, filename):\n",
    "\tresponse = requests.get(url)\n",
    "\tcontent = response.text\n",
    "\tsavetoTXTfile(filename, content)\n",
    "\n",
    "def crawlusingfile(filename):\n",
    "\tfilecontent = readfromfile(filename)\n",
    "\tdoc = lxml.html.fromstring(filecontent)\n",
    "\tnodepath = \"//div[@class='textimage section']//div[@class='text']\"\n",
    "\tSFU_HOMEPAGE = 'http://www.sfu.ca'\n",
    "\tprof_els = doc.xpath(nodepath)\n",
    "\tsfu_prof_records = []\n",
    "\n",
    "\tfor prof_el in prof_els:\n",
    "\t\tname = ''\n",
    "\t\tdesignation = ''\n",
    "\t\tprofile = ''\n",
    "\t\thomepage = ''\n",
    "\t\tresearch_interests = []\n",
    "\n",
    "\t\tfor children in prof_el.getiterator():\n",
    "\t\t\tif(children.tag  == 'h4'):\n",
    "\t\t\t\tprof = (children.text_content().split('\\n'))[0].split(',')\n",
    "\t\t\t\tprof_name = prof[0]\n",
    "\t\t\t\tname = prof_name.title()\n",
    "\t\t\t\tprof_designation = prof[1]\n",
    "\t\t\t\tprof_designation = prof_designation.strip()\n",
    "\t\t\t\tdesignation = prof_designation.title()\n",
    "\t\t\t\tif name == 'Associate Director': continue\n",
    "\t\t\t\t# print('Name: ' + name)\n",
    "\t\t\t\t# print('Designation:' + prof_designation.title())\n",
    "\n",
    "\t\t\telif (children.tag == 'p'):\n",
    "\t\t\t\tfor profile_details in children.getiterator():\n",
    "\t\t\t\t\tlink = profile_details.get('href')\n",
    "\t\t\t\t\ttitle = profile_details.text_content()\n",
    "\t\t\t\t\tif title == 'Profile & Contact Information' and link is not None:\n",
    "\t\t\t\t\t\tif not link.startswith(SFU_HOMEPAGE): link = SFU_HOMEPAGE + link\n",
    "\t\t\t\t\t\t# print('Profile:' + link)\n",
    "\t\t\t\t\t\tprofile = link\n",
    "\t\t\t\t\t\tresearch_interests = getinterests(profile)\n",
    "\t\t\t\t\t\tif not research_interests: research_interests = []\n",
    "\t\t\t\t\t\t# print(\"Interests: {}\".format(research_interests))\n",
    "\t\t\t\t\telif (title == 'Home Page'):\n",
    "\t\t\t\t\t\thomepage = link\n",
    "\t\t\t\t\t\t# print('HomePage:' + homepage)\n",
    "\t\t\n",
    "\t\trow_tuple = (name, designation, profile, homepage, research_interests)\n",
    "\t\tsfu_prof_records.append(row_tuple)\n",
    "\n",
    "\tsavetoCSVfile('faculty_more_table.csv', sfu_prof_records)\n",
    "\n",
    "def main():\n",
    "\tfilename = 'faculty.txt'\n",
    "\tfaculty_page = 'https://www.sfu.ca/computing/people/faculty.html'\n",
    "\tmakelocalcopyofpage(faculty_page, filename)\n",
    "\tcrawlusingfile(filename)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Complete the code in this [notebook](A1.ipynb), and submit it to the CourSys activity [Assignment 1](https://courses.cs.sfu.ca/2018sp-cmpt-733-g1/+a1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
